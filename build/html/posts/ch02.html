<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>第二章 &mdash; In the Cloud alpha documentation</title>
    
    <link rel="stylesheet" href="../_static/haiku.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     'alpha',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="In the Cloud alpha documentation" href="../index.html" />
    <link rel="next" title="第三章" href="ch03.html" />
    <link rel="prev" title="第一章" href="ch01.html" /> 
  </head>
  <body>
      <div class="header"><h1 class="heading"><a href="../index.html">
          <span>In the Cloud alpha documentation</span></a></h1>
        <h2 class="heading"><span>第二章</span></h2>
      </div>
      <div class="topnav">
      
        <p>
        «&#160;&#160;<a href="ch01.html">第一章</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="../index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="ch03.html">第三章</a>&#160;&#160;»
        </p>

      </div>
      <div class="content">
        
        
  <div class="section" id="id1">
<h1>第二章<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id2">
<h2>2.1 谈谈分布式存储<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>计算机领域中有诸多有意思的东西可以把玩，在这儿且看看分布式存储。</p>
<p><strong>集群文件系统</strong></p>
<p>在某些场景下又可以称作网络文件系统、并行文件系统，在70年代由IBM提出并实现原型。</p>
<p>有几种方法可以实现集群形式，但多数仅仅是节点直连存储而不是将存储之上的文件系统进行合理“分布”。分布式文件系统同时挂载于多个服务器上，并在它们之间共享，可以提供类似于位置无关的数据定位或冗余等特点。并行文件系统是一种集群式的文件系统，它将数据分布于多个节点，其主要目的是提供冗余和提高读写性能。</p>
<p><strong>共享磁盘（Shared-disk）/Storage-area network(SAN)</strong></p>
<p>从应用程序使用的文件级别，到SAN之间的块级别的操作，诸如权限控制和传输，都是发生在客户端节点上。共享磁盘（Shared-disk）文件系统，在并行控制上做了很多工作，以至于其拥有比较一致连贯的文件系统视图，从而避免了多个客户端试图同时访问同一设备时数据断片或丢失的情况发生。其中有种技术叫做围栏（Fencing），就是在某个或某些节点发生断片时，集群自动将这些节点隔离（关机、断网、自恢复），保证其他节点数据访问的正确性。元数据（Metadata）类似目录，可以让所有的机器都能查找使用所有信息，在不同的架构中有不同的保存方式，有的均匀分布于集群，有的存储在中央节点。</p>
<p>实现的方式有iSCSI，AoE，FC，Infiniband等，比较著名的产品有Redhat GFS、Sun QFS、Vmware VMFS等。</p>
<p><strong>分布式文件系统</strong></p>
<p>分布式文件系统则不是块级别的共享的形式了，所有加进来的存储（文件系统）都是整个文件系统的一部分，所有数据的传输也是依靠网络来的。</p>
<p>它的设计有这么几个原则：</p>
<ul class="simple">
<li><em>访问透明</em> 客户端在其上的文件操作与本地文件系统无异</li>
<li><em>位置透明</em> 其上的文件不代表其存储位置，只要给了全名就能访问</li>
<li><em>并发透明</em> 所有客户端持有的文件系统的状态在任何时候都是一致的，不会出现A修改了F文件，但是B愣了半天才发现。</li>
<li><em>失败透明</em> 理解为阻塞操作，不成功不回头。</li>
<li><em>异构性</em> 文件系统可以在多种硬件以及操作系统下部署使用。</li>
<li><em>扩展性</em> 随时添加进新的节点，无视其资格新旧。</li>
<li><em>冗余透明</em> 客户端不需要了解文件存在于多个节点上这一事实。</li>
<li><em>迁移透明</em> 客户端不需要了解文件根据负载均衡策略迁移的状况。</li>
</ul>
<p>实现的方式有NFS、CIFS、SMB、NCP等，比较著名的产品有Google GFS、Hadoop HDFS、GlusterFS、Lustre等。</p>
<blockquote class="epigraph">
<div><p>FUSE，filesystem in user space。</p>
<p>FUSE全名Filesystem in Userspace，是在类UNIX系统下的一个机制，可以让普通用户创建修改访问文件系统。功能就是连接内核接口与用户控件程序的一座“桥”，目前普遍存在于多个操作系统中，比如Linux、BSD、Solaris、OSX、Android等。</p>
<p>FUSE来源于AVFS，不同于传统文件系统从磁盘读写数据，FUSE在文件系统或磁盘中有“转换”的角色，本身并不会存储数据。</p>
<p>在Linux系统中的实现有很多，比如各种要挂载ntfs文件系统使用到的ntfs-3g，以及即将要用到的glusterfs-fuse。</p>
</div></blockquote>
<img alt="../_images/02-01.png" class="align-center" src="../_images/02-01.png" />
</div>
<div class="section" id="glusterfs">
<h2>2.2 Glusterfs简述<a class="headerlink" href="#glusterfs" title="Permalink to this headline">¶</a></h2>
<p>接下来，说一下我所看到的glusterfs。</p>
<p>首先它可以基于以太网或者Infiniband构建大规模分布式文件系统，其设计原则符合奥卡姆剃刀原则，即“ <em>若无必要，勿增实体</em> ”；它的源码部分遵循GPLv3，另一部分遵循GPLv2/LGPLv3；统一对象视图，与UNIX设计哲学类似，所有皆对象；跨平台兼容性高，可作为hadoop、openstack、ovirt、Amazon EC的后端。</p>
<img alt="../_images/02-02.png" class="align-center" src="../_images/02-02.png" />
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>砖块（brick）</strong>：即服务器节点上导出的一个目录，作为glusterfs的最基本单元。</p>
<p><strong>卷（volume）</strong>：用户最终使用的、由砖块组成的逻辑卷。</p>
<p><strong>GFID</strong>：glusterfs中的每一个文件或者目录都有一个独立的128位GFID，与普通文件系统中的inode类似。</p>
<p class="last"><strong>节点（peer）</strong>：即集群中含有砖块并参与构建卷的计算机。</p>
</div>
<div class="section" id="id3">
<h3>功能特性<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p><strong>gluster支持四种存储逻辑卷组合：普通分布式（Distributed）、条带（Striped）、冗余（Replicated）、条带冗余（Striped-Replicated）</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="26%" />
<col width="74%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>普通分布式</td>
<td><img alt="../_images/02-04.png" class="first last align-center" src="../_images/02-04.png" />
</td>
</tr>
<tr class="row-even"><td>条带</td>
<td><img alt="../_images/02-05.png" class="first last align-center" src="../_images/02-05.png" />
</td>
</tr>
<tr class="row-odd"><td>冗余</td>
<td><img alt="../_images/02-06.png" class="first last align-center" src="../_images/02-06.png" />
</td>
</tr>
<tr class="row-even"><td>条带冗余</td>
<td><img alt="../_images/02-07.png" class="first last align-center" src="../_images/02-07.png" />
</td>
</tr>
</tbody>
</table>
<p><strong>支持跨网备份</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="26%" />
<col width="74%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>局域网备份</td>
<td><img alt="../_images/02-08.png" class="first last align-center" src="../_images/02-08.png" />
</td>
</tr>
<tr class="row-even"><td>内网备份</td>
<td><img alt="../_images/02-09.png" class="first last align-center" src="../_images/02-09.png" />
</td>
</tr>
<tr class="row-odd"><td>广域网备份</td>
<td><img alt="../_images/02-10.png" class="first last align-center" src="../_images/02-10.png" />
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="id4">
<h2>2.3 搭建Glusterfs作为基础存储<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>既然要搭建一个稳健的基础存储，那么glusterfs推荐使用distributed striped replicated方式，这里使用4台预装CentOS 6(SELINUX设置为permissive)的机器进行演示。</p>
<div class="section" id="dnshosts">
<h3>添加DNS或者修改hosts文件<a class="headerlink" href="#dnshosts" title="Permalink to this headline">¶</a></h3>
<p>鉴于笔者所在环境中暂时没有配置独立的DNS，此处先修改hosts文件以完成配置，注意每台机器都要添加：</p>
<blockquote>
<div><em>/etc/hosts</em></div></blockquote>
<div class="code highlight-python"><div class="highlight"><pre>127.0.0.1       localhost.localdomain localhost
::1             localhost6.localdomain6 localhost6

192.168.10.101  gs1.example.com
192.168.10.102  tgs2.example.com
192.168.10.103  tgs3.example.com
192.168.10.104  gs4.example.com
</pre></div>
</div>
<p>同样地在所有机器上添加repo：</p>
<blockquote>
<div><em>/etc/yum.repos.d/gluster_epel.repo</em></div></blockquote>
<div class="code highlight-python"><div class="highlight"><pre>[epel]
name=Extra Packages for Enterprise Linux 6 - $basearch
#baseurl=http://download.fedoraproject.org/pub/epel/6/$basearch
mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-6&amp;amp;arch=$basearch
failovermethod=priority
enabled=1
gpgcheck=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6

[glusterfs-epel]
name=GlusterFS is a clustered file-system capable of scaling to several petabytes.
baseurl=http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/epel-$releasever/$basearch/
enabled=1
skip_if_unavailable=1
gpgcheck=0
gpgkey=http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/pub.key

[glusterfs-noarch-epel]
name=GlusterFS is a clustered file-system capable of scaling to several petabytes.
baseurl=http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/epel-$releasever/noarch
enabled=1
skip_if_unavailable=1
gpgcheck=0
gpgkey=http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/pub.key

[glusterfs-source-epel]
name=GlusterFS is a clustered file-system capable of scaling to several petabytes. - Source
baseurl=http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/epel-$releasever/SRPMS
enabled=0
skip_if_unavailable=1
gpgcheck=1
gpgkey=http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/pub.key
</pre></div>
</div>
</div>
<div class="section" id="id5">
<h3>准备磁盘作为砖块<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>在所有节点上安装xfs用户空间工具：</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># yum install -y glusterfs glusterfs-fuse glusterfs-server xfsprogs</span>
<span class="c"># /etc/init.d/glusterd start</span>
<span class="c"># /etc/init.d/glusterfsd start</span>
<span class="c"># chkconfig glusterfsd on</span>
<span class="c"># chkconfig glusterd on</span>
</pre></div>
</div>
<p>假设每台机器除系统盘之外都有2块1T SATA硬盘，我们需要对其进行分区，创建逻辑卷，格式化并挂载：</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># fdisk /dev/sdX &lt;&lt; EOF</span>
<span class="n">n</span>
<span class="n">p</span>
<span class="mi">1</span>

<span class="n">w</span>
<span class="n">EOF</span>
</pre></div>
</div>
<p>格式化并挂载：</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># mkfs.xfs -i size 512 /dev/sdb1</span>
<span class="c"># mkfs.xfs -i size 512 /dev/sdc1</span>
<span class="c"># mkdir /gluster_brick_root1</span>
<span class="c"># mkdir /gluster_brick_root2</span>
<span class="c"># echo -e &quot;/dev/sdb1\t/gluster_brick_root1\txfs\tdefaults\t0 0\n/dev/sdc1\t/gluster_brick_root2\txfs\tdefaults\t0 0&quot; &gt;&gt; /etc/fstab</span>
<span class="c"># mount -a</span>
<span class="c"># mkdir /gluster_brick_root1/data</span>
<span class="c"># mkdir /gluster_brick_root2/data</span>
</pre></div>
</div>
<blockquote class="epigraph">
<div><p>为什么要用XFS？</p>
<p>XFS具有元数据日志功能，可以快速恢复数据；同时，可以在线扩容及碎片整理。其他文件系统比如EXT3，EXT4未做充分测试。</p>
</div></blockquote>
</div>
<div class="section" id="id6">
<h3>添加卷<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>在其中任意台机器上，比如gs2.example.com，执行</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># gluster peer probe gs1.example.com</span>
<span class="c"># gluster peer probe gs3.example.com</span>
<span class="c"># gluster peer probe gs4.example.com</span>
</pre></div>
</div>
<p>使用砖块进行卷的构建：</p>
<div class="code highlight-python"><div class="highlight"><pre># gluster
  &gt; volume create gluster-vol1 stripe 2 replica 2 \
  gs1.example.com:/gluster_brick_root1/data gs2.example.com:/gluster_brick_root1/data \
  gs1.example.com:/gluster_brick_root2/data gs2.example.com:/gluster_brick_root2/data \
  gs3.example.com:/gluster_brick_root1/data gs4.example.com:/gluster_brick_root1/data \
  gs3.example.com:/gluster_brick_root2/data gs4.example.com:/gluster_brick_root2/data force
  &gt; volume start gluster-vol1 # 启动卷
  &gt; volume status gluster-vol1 # 查看卷状态
  Status of volume: gluster-vol1
  Gluster process                                         Port    Online  Pid
  ------------------------------------------------------------------------------
  Brick gs1.example.com:/gluster_brick_root1/data         49152   Y       1984
  Brick gs2.example.com:/gluster_brick_root1/data         49152   Y       1972
  Brick gs1.example.com:/gluster_brick_root2/data         49153   Y       1995
  Brick gs2.example.com:/gluster_brick_root2/data         49153   Y       1983
  Brick gs3.example.com:/gluster_brick_root1/data         49152   Y       1961
  Brick gs4.example.com:/gluster_brick_root1/data         49152   Y       1975
  Brick gs3.example.com:/gluster_brick_root2/data         49153   Y       1972
  Brick gs4.example.com:/gluster_brick_root2/data         49153   Y       1986
  NFS Server on localhost                                 2049    Y       1999
  Self-heal Daemon on localhost                           N/A     Y       2006
  NFS Server on gs2.example.com                           2049    Y       2007
  Self-heal Daemon on gs2.example.com                     N/A     Y       2014
  NFS Server on gs2.example.com                           2049    Y       1995
  Self-heal Daemon on gs2.example.com                     N/A     Y       2002
  NFS Server on gs3.example.com                           2049    Y       1986
  Self-heal Daemon on gs3.example.com                     N/A     Y       1993

  Task Status of Volume gluster-vol1
  ------------------------------------------------------------------------------
  There are no active volume tasks
  &gt; volume info all 查看所有卷信息
  gluster volume info all

  Volume Name: gluster-vol1
  Type: Distributed-Striped-Replicate
  Volume ID: bc8e102c-2b35-4748-ab71-7cf96ce083f3
  Status: Started
  Number of Bricks: 2 x 2 x 2 = 8
  Transport-type: tcp
  Bricks:
  Brick1: gs1.example.com:/gluster_brick_root1/data
  Brick2: gs2.example.com:/gluster_brick_root1/data
  Brick3: gs1.example.com:/gluster_brick_root2/data
  Brick4: gs2.example.com:/gluster_brick_root2/data
  Brick5: gs3.example.com:/gluster_brick_root1/data
  Brick6: gs4.example.com:/gluster_brick_root1/data
  Brick7: gs3.example.com:/gluster_brick_root2/data
  Brick8: gs4.example.com:/gluster_brick_root2/data
</pre></div>
</div>
</div>
<div class="section" id="id7">
<h3>挂载卷<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>当以glusterfs挂载时，客户端的hosts文件里需要有的任一节点做解析：</p>
<blockquote>
<div><em>挂载glusterfs的客户端/etc/hosts</em></div></blockquote>
<div class="code highlight-python"><div class="highlight"><pre>127.0.0.1       localhost.localdomain localhost
::1             localhost6.localdomain6 localhost6

192.168.1.81    gs1.example.com
</pre></div>
</div>
<p>安装gluster-fuse，将gluster卷作为glusterfs挂载，并写入1M文件查看其在各砖块分配：</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># yum install glusterfs glusterfs-fuse</span>
<span class="c"># mount.glusterfs 192.168.1.81:/gluster-vol1 /mnt</span>
<span class="c"># cd /mnt</span>
<span class="c"># dd if=/dev/zero of=a.img bs=1k count=1k</span>
<span class="c"># cp a.img b.img; cp a.img c.img; cp a.img d.img</span>
</pre></div>
</div>
<p>在四台服务端分别查看：</p>
<div class="code highlight-python"><div class="highlight"><pre>[root@gs1 ~]# ls -lh /gluster_brick_root*
/gluster_brick_root1/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 a.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 d.img
/gluster_brick_root2/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 a.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 d.img
</pre></div>
</div>
<div class="code highlight-python"><div class="highlight"><pre>[root@gs2 ~]# ls -lh /gluster_brick_root*
/gluster_brick_root1/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 a.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 d.img
/gluster_brick_root2/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 a.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 d.img
</pre></div>
</div>
<div class="code highlight-python"><div class="highlight"><pre>[root@gs3 ~]# ls -lh /gluster_brick_root*
/gluster_brick_root1/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 b.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 c.img
/gluster_brick_root2/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 b.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 c.img
</pre></div>
</div>
<div class="code highlight-python"><div class="highlight"><pre>[root@gs4 ~]# ls -lh /gluster_brick_root*
/gluster_brick_root1/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 b.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 c.img
/gluster_brick_root2/data/:
total 1.0M
-rw-r--r--. 2 root root 512K Apr 22 17:13 b.img
-rw-r--r--. 2 root root 512K Apr 22 17:13 c.img
</pre></div>
</div>
<p>至此，所有配置结束。</p>
</div>
</div>
<div class="section" id="id8">
<h2>2.4 Glusterfs应用示例及技巧<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id9">
<h3>文件权限<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>glusterfs在创建卷时会更改砖块所有者为root.root，对于某些应用请注意更改砖块目录所有者（比如在/etc/rc.local中添加chown，不要更改砖块下隐藏目录.glusterfs）。</p>
</div>
<div class="section" id="id10">
<h3>砖块组合<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>网上现有的部分文档中所述的砖块划分方式，是将整个磁盘划分为砖块，此种划分方式在某些场景下不是很好（比如存储复用），可以在/brickX下创建目录，比如data1，同时在创建glusterfs卷的时候使用HOST:/brickX/data1作为砖块，以合理利用存储空间。</p>
</div>
<div class="section" id="normalreplicastriped">
<h3>normal、replica、striped卷组合<a class="headerlink" href="#normalreplicastriped" title="Permalink to this headline">¶</a></h3>
<p>砖块的划分排序：striped（normal）优先，replica在striped（normal）基础上做冗余；计算大小时，同一replica组中的brick合并为一个砖块，一个striped组可看做一个有效块。</p>
<p>假设我们有4个主机，8个砖块，每个砖块都是5GB，如下图：</p>
<blockquote>
<div><img alt="../_images/02-11.png" class="align-center" src="../_images/02-11.png" />
</div></blockquote>
<p>创建卷时使用如下命令：</p>
<div class="code highlight-python"><div class="highlight"><pre># gluster volume create gluster-vol1 stripe 2 replica 2 \
host1:/brick1 host1:/brick2 host2:/brick1 host2:/brick2 \
host3:/brick1 host3:/brick2 host4:/brick1 host4:/brick2 force
</pre></div>
</div>
<p>砖块将会按照如下进行组合：</p>
<blockquote>
<div><img alt="../_images/02-12.png" class="align-center" src="../_images/02-12.png" />
</div></blockquote>
<p>然而，创建卷时使用如下命令：</p>
<div class="code highlight-python"><div class="highlight"><pre># gluster volume create gluster-vol1 stripe 2 replica 2 \
host1:/brick1 host2:/brick1 host3:/brick1 host4:/brick1 \
host1:/brick2 host2:/brick2 host3:/brick2 host4:/brick2 force
</pre></div>
</div>
<p>砖块将会按照如下进行组合：</p>
<blockquote>
<div><img alt="../_images/02-13.png" class="align-center" src="../_images/02-13.png" />
</div></blockquote>
</div>
<div class="section" id="nfs">
<h3>作为nfs挂载<a class="headerlink" href="#nfs" title="Permalink to this headline">¶</a></h3>
<p>由于glusterfs占用了2049端口，所以其与nfs server一般不能共存于同一台服务器，除非更改nfs服务端口。</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># mount -t nfs -o vers=3 server1:/volume1 /mnt</span>
</pre></div>
</div>
</div>
<div class="section" id="cifs">
<h3>作为cifs挂载<a class="headerlink" href="#cifs" title="Permalink to this headline">¶</a></h3>
<p>先在某一服务器或者客户端将起挂载，再以cifs方式导出：</p>
<blockquote>
<div>/etc/smb.conf</div></blockquote>
<div class="code highlight-python"><div class="highlight"><pre>[glustertest]
comment = For testing a Gluster volume exported through CIFS
path = /mnt/glusterfs
read only = no
guest ok = yes
</pre></div>
</div>
</div>
<div class="section" id="split-brain">
<h3>修复裂脑（split-brain）<a class="headerlink" href="#split-brain" title="Permalink to this headline">¶</a></h3>
<p>裂脑发生以后，各节点信息可能会出现不一致。可以通过以下步骤查看并修复。</p>
<ol class="arabic simple">
<li>定位裂脑文件</li>
</ol>
<p>通过命令</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># gluster volume heal info split-brain</span>
</pre></div>
</div>
<p>或者查看在客户端仍然是Input/Output错误的文件。</p>
<ol class="arabic simple" start="2">
<li>关闭已经打开的文件或者虚机</li>
<li>确定正确副本</li>
<li>恢复扩展属性</li>
</ol>
</div>
<div class="section" id="id11">
<h3>砖块复用<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>当卷正在被使用，其中一个砖块被删除，而用户试图再次将其用于卷时，可能会出现“/bricks/app or a prefix of it is already part of a volume”。</p>
<p>解决方法：</p>
<div class="code highlight-python"><div class="highlight"><pre><span class="c"># setfattr -x trusted.glusterfs.volume-id $brick_path</span>
<span class="c"># setfattr -x trusted.gfid $brick_path</span>
<span class="c"># rm -rf $brick_path/.glusterfs</span>
</pre></div>
</div>
</div>
</div>
</div>


      </div>
      <div class="bottomnav">
      
        <p>
        «&#160;&#160;<a href="ch01.html">第一章</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="../index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="ch03.html">第三章</a>&#160;&#160;»
        </p>

      </div>

    <div class="footer">
        &copy; Copyright 2014, lofyer.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.2.
    </div>
  </body>
</html>